
https://www.top500.org/lists/top500/list/2021/11/

# [Voyager](https://www.top500.org/system/180024)[-](https://www.top500.org/system/180024)[EUS](https://www.top500.org/system/180024)[2](https://www.top500.org/system/180024) - 10

#10:

[Voyager](https://www.top500.org/system/180024)[-](https://www.top500.org/system/180024)[EUS](https://www.top500.org/system/180024)[2 - ](https://www.top500.org/system/180024)[ND](https://www.top500.org/system/180024)[96](https://www.top500.org/system/180024)[amsr](https://www.top500.org/system/180024)[_](https://www.top500.org/system/180024)[A](https://www.top500.org/system/180024)[100_](https://www.top500.org/system/180024)[v](https://www.top500.org/system/180024)[4, ](https://www.top500.org/system/180024)[AMD](https://www.top500.org/system/180024)[ ](https://www.top500.org/system/180024)[EPYC](https://www.top500.org/system/180024)[ 7](https://www.top500.org/system/180024)[V](https://www.top500.org/system/180024)[12 48](https://www.top500.org/system/180024)[C](https://www.top500.org/system/180024)[ 2.45](https://www.top500.org/system/180024)[GHz](https://www.top500.org/system/180024)[, ](https://www.top500.org/system/180024)[NVIDIA](https://www.top500.org/system/180024)[ ](https://www.top500.org/system/180024)[A](https://www.top500.org/system/180024)[100 80](https://www.top500.org/system/180024)[GB](https://www.top500.org/system/180024)[​, ](https://www.top500.org/system/180024)[Mellanox](https://www.top500.org/system/180024)[ ](https://www.top500.org/system/180024)[HDR](https://www.top500.org/system/180024)[ ](https://www.top500.org/system/180024)[Infiniband](https://www.top500.org/system/180024)[, ](https://www.top500.org/system/180024)Microsoft Azure

[Azure](https://www.top500.org/site/50897)[ ](https://www.top500.org/site/50897)[East](https://www.top500.org/site/50897)[ ](https://www.top500.org/site/50897)[US](https://www.top500.org/site/50897)[ 2](https://www.top500.org/site/50897)

United States

[https](https://www.top500.org/system/180024/)[://](https://www.top500.org/system/180024/)[www](https://www.top500.org/system/180024/)[.](https://www.top500.org/system/180024/)[top](https://www.top500.org/system/180024/)[500.](https://www.top500.org/system/180024/)[org](https://www.top500.org/system/180024/)[/](https://www.top500.org/system/180024/)[system](https://www.top500.org/system/180024/)[/180024/](https://www.top500.org/system/180024/)

[https](https://en.wikipedia.org/wiki/Voyager-EUS2)[://](https://en.wikipedia.org/wiki/Voyager-EUS2)[en](https://en.wikipedia.org/wiki/Voyager-EUS2)[.](https://en.wikipedia.org/wiki/Voyager-EUS2)[wikipedia](https://en.wikipedia.org/wiki/Voyager-EUS2)[.](https://en.wikipedia.org/wiki/Voyager-EUS2)[org](https://en.wikipedia.org/wiki/Voyager-EUS2)[/](https://en.wikipedia.org/wiki/Voyager-EUS2)[wiki](https://en.wikipedia.org/wiki/Voyager-EUS2)[/](https://en.wikipedia.org/wiki/Voyager-EUS2)[Voyager](https://en.wikipedia.org/wiki/Voyager-EUS2)[-](https://en.wikipedia.org/wiki/Voyager-EUS2)[EUS](https://en.wikipedia.org/wiki/Voyager-EUS2)[2](https://en.wikipedia.org/wiki/Voyager-EUS2)

[https](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[://](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[redmondmag](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[.](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[com](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[/](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[articles](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[/2021/11/17/](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[microsoft](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[azure](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[based](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[machine](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[ranked](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[no](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[.-10-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[in](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[supercomputing](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[-](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[list](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[.](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)[aspx](https://redmondmag.com/articles/2021/11/17/microsoft-azure-based-machine-ranked-no.-10-in-supercomputing-list.aspx)

[https](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[://](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[ummid](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[.](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[com](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[/](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[news](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[/2020/](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[may](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[/20.05.2020/](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[microsoft](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[launches](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[new](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[supercomputer](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[to](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[train](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[large](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[ai](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[-](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[models](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[.](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)[html](https://ummid.com/news/2020/may/20.05.2020/microsoft-launches-new-supercomputer-to-train-large-ai-models.html)

[https](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[://](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[analyticsindiamag](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[.](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[com](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[/](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[how](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[the](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[partnership](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[with](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[openai](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[has](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[cemented](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[microsofts](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[position](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[in](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[-](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[supercomputers](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)[/](https://analyticsindiamag.com/how-the-partnership-with-openai-has-cemented-microsofts-position-in-supercomputers/)

[https](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[://](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[ncses](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[.](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[nsf](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[.](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[gov](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[pubs](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[nsb](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[20226/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[assets](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[enabling](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[technologies](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[sidebars](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[/](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[nsb](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[20226-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[trends](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[in](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[high](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[performance](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[-](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[computing](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[.](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)[pdf](https://ncses.nsf.gov/pubs/nsb20226/assets/enabling-technologies/sidebars/nsb20226-trends-in-high-performance-computing.pdf)

Only cited twice, once it :

[https](https://www.mdpi.com/2079-9292/11/9/1369)[://](https://www.mdpi.com/2079-9292/11/9/1369)[www](https://www.mdpi.com/2079-9292/11/9/1369)[.](https://www.mdpi.com/2079-9292/11/9/1369)[mdpi](https://www.mdpi.com/2079-9292/11/9/1369)[.](https://www.mdpi.com/2079-9292/11/9/1369)[com](https://www.mdpi.com/2079-9292/11/9/1369)[/2079-9292/11/9/1369](https://www.mdpi.com/2079-9292/11/9/1369)


# JUWELS - 8


|8|<p>[JUWELS](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[Booster](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[Module](https://www.top500.org/system/179894)[ - ](https://www.top500.org/system/179894)[Bull](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[Sequana](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[XH](https://www.top500.org/system/179894)[2000 , ](https://www.top500.org/system/179894)[AMD](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[EPYC](https://www.top500.org/system/179894)[ 7402 24](https://www.top500.org/system/179894)[C](https://www.top500.org/system/179894)[ 2.8](https://www.top500.org/system/179894)[GHz](https://www.top500.org/system/179894)[, ](https://www.top500.org/system/179894)[NVIDIA](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[A](https://www.top500.org/system/179894)[100, ](https://www.top500.org/system/179894)[Mellanox](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[HDR](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[InfiniBand](https://www.top500.org/system/179894)[/](https://www.top500.org/system/179894)[ParTec](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[ParaStation](https://www.top500.org/system/179894)[ ](https://www.top500.org/system/179894)[ClusterSuite](https://www.top500.org/system/179894)[, ](https://www.top500.org/system/179894)Atos</p><p>[Forschungszentrum](https://www.top500.org/site/47871)[ ](https://www.top500.org/site/47871)[Juelich](https://www.top500.org/site/47871)[ (](https://www.top500.org/site/47871)[FZJ](https://www.top500.org/site/47871)[)](https://www.top500.org/site/47871)</p><p>Germany</p>|
| :- | :- |
[https](https://www.top500.org/system/179894/)[://](https://www.top500.org/system/179894/)[www](https://www.top500.org/system/179894/)[.](https://www.top500.org/system/179894/)[top](https://www.top500.org/system/179894/)[500.](https://www.top500.org/system/179894/)[org](https://www.top500.org/system/179894/)[/](https://www.top500.org/system/179894/)[system](https://www.top500.org/system/179894/)[/179894/](https://www.top500.org/system/179894/)

[https](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[://](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[dl](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[.](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[acm](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[.](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[org](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[/](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[doi](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[/](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[abs](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)[/10.1145/3452412.3462752](https://dl.acm.org/doi/abs/10.1145/3452412.3462752)

JUWELS Booster - Early User Experiences

@inproceedings{10.1145/3452412.3462752,

author = {Herten, Andreas},

title = {JUWELS Booster - Early User Experiences},

year = {2021},

isbn = {9781450383875},

publisher = {Association for Computing Machinery},

address = {New York, NY, USA},

url = {https://doi.org/10.1145/3452412.3462752},

doi = {10.1145/3452412.3462752},

abstract = {Over the last few years, GPUs became ubiquitous in HPC installations around the world. Today, they provide the main source of performance in a number of Top500 machines - for example Summit, Sierra, and JUWELS Booster. Also for the upcoming Exascale era, GPUs are selected as key enablers and will be installed numerously. While individual GPU devices already offer plenty of performance (O (10) TFLOP/sFP64), current and next-generation super-computers employ them in the thousands. Using these machines to the fullest extend means not only utilizing individual devices efficiently, but using the entire interconnected system of devices thoroughly.JUWELS Booster is a recently installed Tier-0/1 system at J\"{u}lich Supercomputing Centre (JSC), currently the 7th-fastest supercomputer in the world, and the fastest in Europe. JUWELS Booster features 936 nodes, each equipped with 4 NVIDIA A100 Tensor Core GPUs and 4 Mellanox HDR200 InfiniBand HCAs. The peak performance of all GPUs together sums up to 73 PFLOP/s and it features a DragonFly+ network topology with 800 Gbit/s network injection bandwidth per node.During installation of JUWELS Booster, a selected set of applications were given access to the system as part of the JUWELS Booster Early Access Program. To prepare for their first compute time allocation, scientific users were able to gain first experiences on the machine. They gave direct feedback to the system operations team during installation and beyond. Close collaboration was facilitated with the application support staff of JSC, giving unique insights into the individual processes of utilizing a brand-new large-sale system for a first time. Likewise, performance profiles of applications could be studied and collaboratively analyzed, employing available tools and methods. Performance limiters of the specific application on the platform were identified and proposals for improvement developed.This talk will present first experiences with JUWELS Booster and the applications utilizing the system during its first months. Applied methods for onboarding, analysis, and optimization will be shown and assessed. Highlights of the state of the art of performance analysis and modeling for GPUs will be presented with concrete examples from the JUWELS Booster Early Access Program.},

booktitle = {Proceedings of the 2021 on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy},

pages = {1},

numpages = {1},

keywords = {gpu, exascale, parallel computing, gpu computing, profiling, distributed computing},

location = {Virtual Event, Sweden},

series = {PERMAVOST '21}

}

Same article :

[https](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[://](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[juser](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[.](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[fz](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[-](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[juelich](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[.](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[de](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[/](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[record](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[/893756/](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[files](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[/3452412.3462752.](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)[pdf](https://juser.fz-juelich.de/record/893756/files/3452412.3462752.pdf)

Power point of article…:

[https](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[://](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[permavost](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[.](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[github](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[.](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[io](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[/](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[files](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[/](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[aherten](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[-](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[jwbooster](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[-](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[permavost](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[-2021.](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)[pdf](https://permavost.github.io/files/aherten-jwbooster-permavost-2021.pdf)

Actually able to read it:

[https](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[://](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[dl](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[-](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[acm](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[-](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[org](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[.](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[proxy](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[.](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[libraries](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[.](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[smu](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[.](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[edu](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[/](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[doi](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[/](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[epdf](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)[/10.1145/3452412.3462752](https://dl-acm-org.proxy.libraries.smu.edu/doi/epdf/10.1145/3452412.3462752)
## This video seems to have the content… I guess since it’s a keynote:
[https](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[://](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[xdev](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[.](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[andreasherten](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[.](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[de](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[/2021/07/01/](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[permavast](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[-](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[talk](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[.](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)[html](https://xdev.andreasherten.de/2021/07/01/permavast-talk.html)

It is almost 4 and 1/2 hours long. 

[https](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[://](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[www](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[.](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[youtube](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[.](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[com](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[/](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[watch](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[?](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[v](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[=](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[qzQCmgulP](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[2](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[g](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[&](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[t](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[=818](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)[s](https://www.youtube.com/watch?v=qzQCmgulP2g&t=818s)


[https](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[://](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[www](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[.](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[hpcadvisorycouncil](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[.](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[com](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[/](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[events](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[/2012/](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[Switzerland](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[-](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[Workshop](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[/](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[Presentations](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[/](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[Day](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[_3/7_](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[Simula](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[.](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)[pdf](https://www.hpcadvisorycouncil.com/events/2012/Switzerland-Workshop/Presentations/Day_3/7_Simula.pdf)

[https](https://www.osti.gov/servlets/purl/1510703)[://](https://www.osti.gov/servlets/purl/1510703)[www](https://www.osti.gov/servlets/purl/1510703)[.](https://www.osti.gov/servlets/purl/1510703)[osti](https://www.osti.gov/servlets/purl/1510703)[.](https://www.osti.gov/servlets/purl/1510703)[gov](https://www.osti.gov/servlets/purl/1510703)[/](https://www.osti.gov/servlets/purl/1510703)[servlets](https://www.osti.gov/servlets/purl/1510703)[/](https://www.osti.gov/servlets/purl/1510703)[purl](https://www.osti.gov/servlets/purl/1510703)[/1510703](https://www.osti.gov/servlets/purl/1510703)

Design space exploration of the Dragonfly topology

1) Juwels is 2 part system

Juwels cluster, 2018. 48 GPU nodes. Mellanox EDR 100 GBit

12 PFLop

1) Juwels Booster

4 A100 from nvidia (beefy but less)

Mellanox EDGE 200 GBit, dragonfly topology

73 PFlop

Larger, faster memory and l1, l2 cache

By reducing the precision from full 64 bit, to 32 bit go from 73 PFLop to 584 PFlop

Binary - 18.7 EOP (single precision binary)

Think of as having 2 modules. Shared file system between them. Shared workload management

2018 start

2018 top 500 #23

2019 booser kickoff

2020 nov top 500 # 7

Early access program 2020

Invited 14 applications from various scientific domains

## [https](https://gmd.copernicus.org/articles/15/2731/2022/)[://](https://gmd.copernicus.org/articles/15/2731/2022/)[gmd](https://gmd.copernicus.org/articles/15/2731/2022/)[.](https://gmd.copernicus.org/articles/15/2731/2022/)[copernicus](https://gmd.copernicus.org/articles/15/2731/2022/)[.](https://gmd.copernicus.org/articles/15/2731/2022/)[org](https://gmd.copernicus.org/articles/15/2731/2022/)[/](https://gmd.copernicus.org/articles/15/2731/2022/)[articles](https://gmd.copernicus.org/articles/15/2731/2022/)[/15/2731/2022/](https://gmd.copernicus.org/articles/15/2731/2022/)
Hoffmann, L., Baumeister, P. F., Cai, Z., Clemens, J., Griessbach, S., Günther, G., Heng, Y., Liu, M., Haghighi Mood, K., Stein, O., Thomas, N., Vogel, B., Wu, X., and Zou, L.: Massive-Parallel Trajectory Calculations version 2.2 (MPTRAC-2.2): Lagrangian transport simulations on graphics processing units (GPUs), Geosci. Model Dev., 15, 2731–2762, https://doi.org/10.5194/gmd-15-2731-2022, 2022.

Lagrangian models are fundamental tools to study atmospheric transport processes and for practical applications such as dispersion modeling for anthropogenic and natural emission sources. However, conducting large-scale Lagrangian transport simulations with millions of air parcels or more can become rather numerically costly. In this study, we assessed the potential of exploiting graphics processing units (GPUs) to accelerate Lagrangian transport simulations. We ported the Massive-Parallel Trajectory Calculations (MPTRAC) model to GPUs using the open accelerator (OpenACC) programming model. The trajectory calculations conducted within the MPTRAC model were fully ported to GPUs, i.e., except for feeding in the meteorological input data and for extracting the particle output data, the code operates entirely on the GPU devices without frequent data transfers between CPU and GPU memory. Model verification, performance analyses, and scaling tests of the Message Passing Interface (MPI) – Open Multi-Processing (OpenMP) – OpenACC hybrid parallelization of MPTRAC were conducted on the Jülich Wizard for European Leadership Science (JUWELS) Booster supercomputer operated by the Jülich Supercomputing Centre, Germany. The JUWELS Booster comprises 3744 NVIDIA A100 Tensor Core GPUs, providing a peak performance of 71.0 PFlop s−1. As of June 2021, it is the most powerful supercomputer in Europe and listed among the most energy-efficient systems internationally. For large-scale simulations comprising 108 particles driven by the European Centre for Medium-Range Weather Forecasts' fifth-generation reanalysis (ERA5), the performance evaluation showed a maximum speed-up of a factor of 16 due to the utilization of GPUs compared to CPU-only runs on the JUWELS Booster. In the large-scale GPU run, about 67 % of the runtime is spent on the physics calculations, conducted on the GPUs. Another 15 % of the runtime is required for file I/O, mostly to read the large ERA5 data set from disk. Meteorological data preprocessing on the CPUs also requires about 15 % of the runtime. Although this study identified potential for further improvements of the GPU code, we consider the MPTRAC model ready for production runs on the JUWELS Booster in its present form. The GPU code provides a much faster time to solution than the CPU code, which is particularly relevant for near-real-time applications of a Lagrangian transport model.

“we ported our existing Lagrangian transport model MPTRAC to GPUs by means of the open accelerator (OpenACC) programming model.”

“As the CPU code of MPTRAC was already verified in earlier studies, we mainly focus on the direct comparisons of CPU and GPU calculations in the present work.”

“All the processing functions (module\_\*) are highlighted in Fig. [1](https://gmd.copernicus.org/articles/15/2731/2022/#Ch1.F1) to indicate that they have been ported to GPUs in the present work. Porting these processing functions to GPUs is the natural choice as these functions typically require most of the computing time, in particular for simulations with many particles. “

“ input and output functions cannot easily be ported to GPUs as the input and output operations are directed over the CPUs and utilize CPU memory in the first place.”

“Lagrangian particle dispersion models are well suited for parallel computing as large sets of air parcel trajectories can be calculated independently of each other. The workload is “embarrassingly parallel” or “perfectly parallel” as little to no effort is needed to separate the problem into a number of independent parallel computing tasks. “

“Open accelerators (OpenACC) is a programming model to enable parallel programming of heterogeneous CPU–GPU systems. As in OpenMP, the source code of the program is annotated with “pragmas” to identify the areas that should be accelerated by using GPUs. In combination with MPI and OpenMP, OpenACC can be used to conduct multi-GPU simulations.”

“CUDA requires solid knowledge on GPU technology and it may cause error-prone and time-consuming work if legacy code needs to be rewritten”

“By using OpenACC, we found that we were able to maintain the same code base of the MPTRAC model rather than having to develop a CPU and a GPU version of the model independently of each other”

“the common code base is considered a significant advantage to check for bit-level agreement of CPU and GPU computations and reproducibility of simulation results.”

“Data transfers between CPUs and GPUs are needed only for updating of the meteorological data or for writing model output.”

“In the original CPU code, the GNU Scientific Library (GSL) was applied to conduct a number of tasks, for instance, to compute statistics of data arrays. As the GSL is not ported to GPUs, the corresponding functions had to be rewritten without usage of the GSL. The GSL was also used to generate random numbers. Here, we implemented NVIDIA's CUDA random number generation library (cuRAND) as an efficient replacement for distributed random number generation on GPUs”

“Overall, the timeline analysis indicates that the code is providing a high utilization rate of the GPU devices, which is a basic requirement for using the GPU devices efficiently. However, the timeline analysis also reveals optimization opportunities. Overlapping file I/O and GPU computations can hide the file I/O costs with the expense of a slight increase in memory usage and reduce the wall-clock time significantly. As the meteorological data are finally required on the GPU devices to conduct the Lagrangian transport simulations, it could also be beneficial to port the meteorological data processing from the CPU to the GPU devices to accelerate the processing.”

“Finally, a more detailed inspection of the physics calculations on the GPUs shows that about 70 % of the time is spent in the advection module, which is used to solve the trajectory equation. A more detailed analysis indicates that this part of the code is memory bound; i.e., the runtime is limited by the GPU's global memory bandwidth. Future work should focus on optimizing this specific bottleneck of the GPU code by improving data locality and memory access patterns on the GPU devices.”

“The MPI scaling test indicates reasonable scaling up to 256 MPI tasks, but it also shows that file I/O will become a bottleneck for larger simulations, at least if large-scale data sets, such as the ERA5 reanalysis, are used to drive the simulations.”


# Frontera - 13
[Frontera](https://www.top500.org/system/179607)[ - ](https://www.top500.org/system/179607)[Dell](https://www.top500.org/system/179607)[ ](https://www.top500.org/system/179607)[C](https://www.top500.org/system/179607)[6420, ](https://www.top500.org/system/179607)[Xeon](https://www.top500.org/system/179607)[ ](https://www.top500.org/system/179607)[Platinum](https://www.top500.org/system/179607)[ 8280 28](https://www.top500.org/system/179607)[C](https://www.top500.org/system/179607)[ 2.7](https://www.top500.org/system/179607)[GHz](https://www.top500.org/system/179607)[, ](https://www.top500.org/system/179607)[Mellanox](https://www.top500.org/system/179607)[ ](https://www.top500.org/system/179607)[InfiniBand](https://www.top500.org/system/179607)[ ](https://www.top500.org/system/179607)[HDR](https://www.top500.org/system/179607)[, ](https://www.top500.org/system/179607)DELL EMC

[Texas](https://www.top500.org/site/48958)[ ](https://www.top500.org/site/48958)[Advanced](https://www.top500.org/site/48958)[ ](https://www.top500.org/site/48958)[Computing](https://www.top500.org/site/48958)[ ](https://www.top500.org/site/48958)[Center](https://www.top500.org/site/48958)[/](https://www.top500.org/site/48958)[Univ](https://www.top500.org/site/48958)[. ](https://www.top500.org/site/48958)[of](https://www.top500.org/site/48958)[ ](https://www.top500.org/site/48958)[Texas](https://www.top500.org/site/48958)

United States

[https](https://www.top500.org/system/179607/)[://](https://www.top500.org/system/179607/)[www](https://www.top500.org/system/179607/)[.](https://www.top500.org/system/179607/)[top](https://www.top500.org/system/179607/)[500.](https://www.top500.org/system/179607/)[org](https://www.top500.org/system/179607/)[/](https://www.top500.org/system/179607/)[system](https://www.top500.org/system/179607/)[/179607/](https://www.top500.org/system/179607/)

Cannot find anything interesting

# Polaris - 12
[Polaris](https://www.top500.org/system/180016)[ - ](https://www.top500.org/system/180016)[Apollo](https://www.top500.org/system/180016)[ 6500, ](https://www.top500.org/system/180016)[AMD](https://www.top500.org/system/180016)[ ](https://www.top500.org/system/180016)[EPYC](https://www.top500.org/system/180016)[ 7532 32](https://www.top500.org/system/180016)[C](https://www.top500.org/system/180016)[ 2.4](https://www.top500.org/system/180016)[GHz](https://www.top500.org/system/180016)[, ](https://www.top500.org/system/180016)[NVIDIA](https://www.top500.org/system/180016)[ ](https://www.top500.org/system/180016)[A](https://www.top500.org/system/180016)[100 ](https://www.top500.org/system/180016)[SXM](https://www.top500.org/system/180016)[4 40 ](https://www.top500.org/system/180016)[GB](https://www.top500.org/system/180016)[, ](https://www.top500.org/system/180016)[Slingshot](https://www.top500.org/system/180016)[-10, ](https://www.top500.org/system/180016)HPE

[DOE](https://www.top500.org/site/47347)[/](https://www.top500.org/site/47347)[SC](https://www.top500.org/site/47347)[/](https://www.top500.org/site/47347)[Argonne](https://www.top500.org/site/47347)[ ](https://www.top500.org/site/47347)[National](https://www.top500.org/site/47347)[ ](https://www.top500.org/site/47347)[Laboratory](https://www.top500.org/site/47347)

United States

Nothing… it’s 2021

Let’s try some older computers…

# Core - 37


|37|<p>[Cori](https://www.top500.org/system/178924)[ - ](https://www.top500.org/system/178924)[Cray](https://www.top500.org/system/178924)[ ](https://www.top500.org/system/178924)[XC](https://www.top500.org/system/178924)[40, ](https://www.top500.org/system/178924)[Intel](https://www.top500.org/system/178924)[ ](https://www.top500.org/system/178924)[Xeon](https://www.top500.org/system/178924)[ ](https://www.top500.org/system/178924)[Phi](https://www.top500.org/system/178924)[ 7250 68](https://www.top500.org/system/178924)[C](https://www.top500.org/system/178924)[ 1.4](https://www.top500.org/system/178924)[GHz](https://www.top500.org/system/178924)[, ](https://www.top500.org/system/178924)[Aries](https://www.top500.org/system/178924)[ ](https://www.top500.org/system/178924)[interconnect](https://www.top500.org/system/178924)[ , ](https://www.top500.org/system/178924)HPE</p><p>[DOE](https://www.top500.org/site/48429)[/](https://www.top500.org/site/48429)[SC](https://www.top500.org/site/48429)[/](https://www.top500.org/site/48429)[LBNL](https://www.top500.org/site/48429)[/](https://www.top500.org/site/48429)[NERSC](https://www.top500.org/site/48429)</p><p>United States</p>|
| :- | :- |
[https](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[://](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[cug](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[.](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[org](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[proceedings](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[cug](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[2014_](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[proceedings](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[includes](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[files](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[pap](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[211.](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)[pdf](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211.pdf)

Cori: A Cray XC Pre-Exascale System for NERSC Katie Antypas, KAntypas@lbl.gov Nicholas Wright, NJWright@lbl.gov Nicholas P. Cardo, NPCardo@lbl.gov Allison Andrews, MNAndrews@lbl.gov Matthew Cordery, MJCordery@lbl.gov National Energy Research Scientific Computing Center Lawrence Berkeley National Laboratory Berkeley, CA 94720 USA Abstract—The next supercomputer for the National Energy Research Scientific Computing Center (NERSC) will be a nextgeneration Cray XC system. The system, named “Cori” after Nobel Laureate Gerty Cori, will bring together technological advances in processors, memory, and storage to enable the solution of the worlds most challenging scientific problems. This new, next-generation Cray XC supercomputer will utilize Intel’s next-generation Intel® Xeon Phi™ processor -- codenamed “Knights Landing” -- a self-hosted, manycore processor with on-package high bandwidth memory and delivering more than 3 teraFLOPS of double-precision peak performance per single socket node. Scheduled for delivery in mid-2016, the new system will deliver 10x the sustained computing capability of NERSC’s Hopper system, a Cray XE6 supercomputer. With the excitement of bringing new technology to bear on worldclass scientific problems also come the many challenges in application development and system management. Strategies to overcome these challenges are key to successful deployment of this system. 

[https](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[://](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[cug](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[.](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[org](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[proceedings](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[cug](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[2014_](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[proceedings](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[includes](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[files](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[/](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[pap](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[211-](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[file](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[2.](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)[pdf](https://cug.org/proceedings/cug2014_proceedings/includes/files/pap211-file2.pdf)

This is from 2014

[https](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[://](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[cug](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[.](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[org](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[/](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[proceedings](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[/](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[cug](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[2016_](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[proceedings](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[/](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[includes](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[/](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[files](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[/](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[pap](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[170](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[s](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[2-](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[file](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[1.](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)[pdf](https://cug.org/proceedings/cug2016_proceedings/includes/files/pap170s2-file1.pdf)

Estimating the Performance Impact of the MCDRAM on KNL Using Dual-Socket Ivy Bridge Nodes on Cray XC30

Zhengji Zhao National Energy Research Scientific Computing Center Lawrence Berkeley National Laboratory Berkeley, USA e-mail: [zzhao](mailto:zzhao@lbl.gov)[@](mailto:zzhao@lbl.gov)[lbl](mailto:zzhao@lbl.gov)[.](mailto:zzhao@lbl.gov)[gov](mailto:zzhao@lbl.gov)

Martijn Marsman Computational Materials Physics University of Vienna Vienna, Austria e-mail: [martijn](mailto:martijn.marsman@univie.ac.at)[.](mailto:martijn.marsman@univie.ac.at)[marsman](mailto:martijn.marsman@univie.ac.at)[@](mailto:martijn.marsman@univie.ac.at)[univie](mailto:martijn.marsman@univie.ac.at)[.](mailto:martijn.marsman@univie.ac.at)[ac](mailto:martijn.marsman@univie.ac.at)[.](mailto:martijn.marsman@univie.ac.at)[at](mailto:martijn.marsman@univie.ac.at)

Abstract— NERSC is preparing for its next petascale system, named Cori, a Cray XC system based on the Intel KNL MIC architecture. Each Cori node will have 72 cores (288 threads), 512 bit vector units, and a low capacity (16GB) and high bandwidth (~5x DDR4) on-package memory (MCDRAM or HBM). To help applications get ready for Cori, NERSC has developed optimization strategies that focus on the MPI+OpenMP program model, vectorization, and the HBM. While the optimization on MPI+OpenMP and vectorization can be carried out on today’s multi-core architectures, optimization of the HBM is difficult to perform where the HBM is unavailable. In this paper, we will present our HBM performance analysis on the VASP code, a widely used materials science code, using Intel's development tools, Memkind and AutoHBW, and a dual-socket Ivy Bridge processor node on Edison, a Cray XC30, as a proxy to the HBM on KNL.

“ If entire NERSC workload can fit into the 16 GB HBM on the KNL nodes, optimization of the HBM would be less of a concern. However, at least 50% of the NERSC workload will not be able to fit into the HBM, as shown in our recent workload analysis [3]. Therefore, the efficient use of the low capacity HBM on the KNL nodes will be an important optimization for applications to get the most performance out of Cori”

“With only a 33% difference in bandwidths, the total run time of the VASP code was reduced by about 40% when all arrays sized from 1M to 5M were allocated to the HBM. Given the fact that the HBM on a KNL node has a bandwidth five times the size of the DDR memory, one could expect a much larger performance boost from utilizing the HBM on KNL. This experiment was very encouraging and motivated the VASP code team to look into HBM optimization during the Dungeon Session (Oct. 2015). To efficiently use the limited amount of the HBM on the KNL nodes, it is critical to allocate only the arrays that generate the most memory traffic to the HBM.”

“About 9% of speedup in the total run time was achieved when a few selected arrays were allocated to the HBM in comparison to when everything was allocated in the DDR memory, while about 14% of speedup was achieved when allocating everything out of the HBM.”


# Dogwood - 47


|<p>[Dogwood](https://www.top500.org/system/179945)[ - ](https://www.top500.org/system/179945)[HPE](https://www.top500.org/system/179945)[ ](https://www.top500.org/system/179945)[Cray](https://www.top500.org/system/179945)[ ](https://www.top500.org/system/179945)[EX](https://www.top500.org/system/179945)[, ](https://www.top500.org/system/179945)[AMD](https://www.top500.org/system/179945)[ ](https://www.top500.org/system/179945)[EPYC](https://www.top500.org/system/179945)[ 7742 64](https://www.top500.org/system/179945)[C](https://www.top500.org/system/179945)[ 2.25](https://www.top500.org/system/179945)[GHz](https://www.top500.org/system/179945)[, ](https://www.top500.org/system/179945)[Slingshot](https://www.top500.org/system/179945)[-10, ](https://www.top500.org/system/179945)HPE</p><p>[GDIT](https://www.top500.org/site/50879)[/](https://www.top500.org/site/50879)[NOAA](https://www.top500.org/site/50879)[/](https://www.top500.org/site/50879)[WCOSS](https://www.top500.org/site/50879)</p><p>United States</p>|
| :- |
Nothing found…


# Artemis - 66


|66|<p>[Artemis](https://www.top500.org/system/179803)[ - ](https://www.top500.org/system/179803)[NVIDIA](https://www.top500.org/system/179803)[ ](https://www.top500.org/system/179803)[DGX](https://www.top500.org/system/179803)[-2, ](https://www.top500.org/system/179803)[Xeon](https://www.top500.org/system/179803)[ ](https://www.top500.org/system/179803)[Platinum](https://www.top500.org/system/179803)[ 8168 24](https://www.top500.org/system/179803)[C](https://www.top500.org/system/179803)[ 2.7](https://www.top500.org/system/179803)[GHz](https://www.top500.org/system/179803)[, ](https://www.top500.org/system/179803)[Infiniband](https://www.top500.org/system/179803)[ ](https://www.top500.org/system/179803)[EDR](https://www.top500.org/system/179803)[, ](https://www.top500.org/system/179803)[NVIDIA](https://www.top500.org/system/179803)[ ](https://www.top500.org/system/179803)[Tesla](https://www.top500.org/system/179803)[ ](https://www.top500.org/system/179803)[V](https://www.top500.org/system/179803)[100, ](https://www.top500.org/system/179803)Nvidia</p><p>[Group](https://www.top500.org/site/50813)[ 42](https://www.top500.org/site/50813)</p><p>United Arab Emirates</p>|
| :- | :- |
This is confusing. This paper refers to Artemis, but seems to be about a different HPC in Sydney Australia…


Vidal, David, et al. “A Parallel Workload Balanced and Memory Efficient Lattice-Boltzmann Algorithm with Single Unit BGK Relaxation Time for Laminar Newtonian Flows.” *Computers & Fluids*, vol. 39, no. 8, Elsevier Ltd, 2010, pp. 1411–23, [https](https://doi.org/10.1016/j.compfluid.2010.04.011)[://](https://doi.org/10.1016/j.compfluid.2010.04.011)[doi](https://doi.org/10.1016/j.compfluid.2010.04.011)[.](https://doi.org/10.1016/j.compfluid.2010.04.011)[org](https://doi.org/10.1016/j.compfluid.2010.04.011)[/10.1016/](https://doi.org/10.1016/j.compfluid.2010.04.011)[j](https://doi.org/10.1016/j.compfluid.2010.04.011)[.](https://doi.org/10.1016/j.compfluid.2010.04.011)[compfluid](https://doi.org/10.1016/j.compfluid.2010.04.011)[.2010.04.011](https://doi.org/10.1016/j.compfluid.2010.04.011).

“It provides perfect parallel workload balance, and its two-nearest-neighbour communication pattern combined with a simple data transfer layout results in 20–55% lower communication cost, 25–60% higher computational parallel performance and 40–90% lower memory usage than previously reported LBM algorithms.”

“parallel efficiencies with 128 processors as high as 75% for domain sizes comprising more than 5 billion fluid nodes.”

“The specific objective of this work is in fact threefold: (1) to highlight the advantages and limitations of the simplified LBM algorithm proposed by Martys and Hagedorn [3] to reduce memory usage, (2) to combine this simplified algorithm with a shift algorithm based on a vector data structure and an even fluid node partitioning domain decomposition, in order to improve parallel performance when dealing with heterogeneous domains, and (3) to compare the memory usage and parallel performance of this novel algorithm with theoretical performance model predictions and one-lattice LBM implementations previously studied”

“Following along the lines of recent other researchers [3–6], a vector data structure is used in this work instead of the sparse matrix data structure inherent to any porous structure, in order to reduce the memory usage by only storing information of the ‘‘fluid nodes” since no computations are performed on the ‘‘solid nodes” (Fig. 2).”

“To alleviate this problem, researchers have devised algorithms that overlap two D. Vidal et al. / Computers & Fluids 39 (2010) 1411–1423 1413 consecutive time steps into one memory array through the use of a clever order of population updates, and thus avoids the overwriting of yet to be propagated populations by ones that have already been. To do so, the one-lattice (two-step) algorithm first executes locally the collisions for all the nodes. It then propagates the resulting populations in ascending node order for the backward-pointing populations and in descending node order for forward-pointing populations. For this to work, it requires the use of so-called (tagged) periodic and bounce-back nodes to store out-going and solid-pointing populations, respectively [20].”

Observation:

Ultimately the simulations described in these papers are of physical phenomena involving huge amounts of observations. Simulation is done at a “cell” level, with interactions with other adjacent cells. This is a highly parallel experiment. The advantage of a computer program doing this is you can get perfect measurements of all the “cells”, and do so without affecting the experiment (vs. putting out 5 trillion thermometers spread across a huge surface, for example).


Let’s try a really old one:


|340|<p>[Inspur](https://www.top500.org/system/179874)[ ](https://www.top500.org/system/179874)[NF](https://www.top500.org/system/179874)[5468](https://www.top500.org/system/179874)[M](https://www.top500.org/system/179874)[5, ](https://www.top500.org/system/179874)[Xeon](https://www.top500.org/system/179874)[ ](https://www.top500.org/system/179874)[Gold](https://www.top500.org/system/179874)[ 5120 14](https://www.top500.org/system/179874)[C](https://www.top500.org/system/179874)[ 2.2](https://www.top500.org/system/179874)[GHz](https://www.top500.org/system/179874)[, ](https://www.top500.org/system/179874)[NVIDIA](https://www.top500.org/system/179874)[ ](https://www.top500.org/system/179874)[Tesla](https://www.top500.org/system/179874)[ ](https://www.top500.org/system/179874)[V](https://www.top500.org/system/179874)[100, 10](https://www.top500.org/system/179874)[G](https://www.top500.org/system/179874)[ ](https://www.top500.org/system/179874)[Ethernet](https://www.top500.org/system/179874)[, ](https://www.top500.org/system/179874)Inspur</p><p>[Bank](https://www.top500.org/site/50846)[ ](https://www.top500.org/site/50846)[I](https://www.top500.org/site/50846)</p><p>China</p>|
| :- | :- |

Nothing



|313|<p>[Snellius](https://www.top500.org/system/180027)[ ](https://www.top500.org/system/180027)[Phase](https://www.top500.org/system/180027)[ 1 ](https://www.top500.org/system/180027)[CPU](https://www.top500.org/system/180027)[ - ](https://www.top500.org/system/180027)[ThinkSystem](https://www.top500.org/system/180027)[ ](https://www.top500.org/system/180027)[SR](https://www.top500.org/system/180027)[645, ](https://www.top500.org/system/180027)[AMD](https://www.top500.org/system/180027)[ ](https://www.top500.org/system/180027)[EPYC](https://www.top500.org/system/180027)[ 7](https://www.top500.org/system/180027)[H](https://www.top500.org/system/180027)[12 64](https://www.top500.org/system/180027)[C](https://www.top500.org/system/180027)[ 2.6](https://www.top500.org/system/180027)[GHz](https://www.top500.org/system/180027)[, ](https://www.top500.org/system/180027)[InfiniBand](https://www.top500.org/system/180027)[ ](https://www.top500.org/system/180027)[HDR](https://www.top500.org/system/180027)[100, ](https://www.top500.org/system/180027)Lenovo</p><p>[SURF](https://www.top500.org/site/50900)</p><p>Netherlands</p>|
| :- | :- |
[https](https://link.springer.com/article/10.1007/s10237-022-01579-0)[://](https://link.springer.com/article/10.1007/s10237-022-01579-0)[link](https://link.springer.com/article/10.1007/s10237-022-01579-0)[.](https://link.springer.com/article/10.1007/s10237-022-01579-0)[springer](https://link.springer.com/article/10.1007/s10237-022-01579-0)[.](https://link.springer.com/article/10.1007/s10237-022-01579-0)[com](https://link.springer.com/article/10.1007/s10237-022-01579-0)[/](https://link.springer.com/article/10.1007/s10237-022-01579-0)[article](https://link.springer.com/article/10.1007/s10237-022-01579-0)[/10.1007/](https://link.springer.com/article/10.1007/s10237-022-01579-0)[s](https://link.springer.com/article/10.1007/s10237-022-01579-0)[10237-022-01579-0](https://link.springer.com/article/10.1007/s10237-022-01579-0)

“In case of a transitional or a turbulent flow, it is always challenging to decide how many total cycles should be used for ensemble averaging until the quantities are statistically converged as the compute costs and physical details need to be balanced. Ideally, the ensemble average should attain the shape of a laminar flow, similar to inflow or the turbulent fluctuations should wash out from the averaged plot.”

How long do you run the program before you accept it has converged?

“. An *abort criteria* was set in *Musubi* to stop the simulation upon achievement of a steady ensemble average. Thus, each subsequent cycle was ensemble averaged with the previously computed cycles and when the velocity fluctuations in ensemble averages vanished, the simulations were stopped automatically. This analysis revealed that a total of about 

n=20

n=20 cycles were enough for the most turbulent simulation, which further implicated the choice of 

n=20

n=20 cycles for all the cases.”

